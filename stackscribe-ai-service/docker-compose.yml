services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: stackscribe-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    restart: unless-stopped
    # Note: We intentionally omit a container-level healthcheck here because the upstream
    # Qdrant image does not reliably include curl/wget, which can produce false "unhealthy"
    # statuses even when the service is running. Our host-level checks (and the Tauri app)
    # validate Qdrant availability via HTTP instead.

  ollama:
    image: ollama/ollama:latest
    container_name: stackscribe-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.
    environment:
      - OLLAMA_KEEP_ALIVE=30m
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_MAX_QUEUE=64
    restart: unless-stopped
  ai-service:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: stackscribe-ai-service
    ports:
      - "8000:8000"
    depends_on:
      - qdrant
    environment:
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - REQ_LLM_BASE_URL=http://ollama:11434/v1
      - REQ_LLM_API_KEY=local-api-key
      - REQ_LLM_MODEL=llama31-8b-instruct-q4k-4k
      - EMBED_MODEL=nomic-ai/nomic-embed-text-v1
      - RERANKER_ID=BAAI/bge-reranker-base
      - THRESHOLD=0.05
      - TOP_K=8
      - COLLECTION=note_chunks
    volumes:
      - ./models:/app/models
      - ./cache:/app/cache
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  qdrant_storage:
    driver: local
  ollama:
    driver: local

networks:
  default:
    name: stackscribe-ai-network 